{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b733d45",
   "metadata": {},
   "source": [
    "## Neural network for detection of AF peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f1e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e3c259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the dataframe: (150000, 31)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data1</th>\n",
       "      <th>data2</th>\n",
       "      <th>data3</th>\n",
       "      <th>data4</th>\n",
       "      <th>data5</th>\n",
       "      <th>data6</th>\n",
       "      <th>data7</th>\n",
       "      <th>data8</th>\n",
       "      <th>data9</th>\n",
       "      <th>data10</th>\n",
       "      <th>...</th>\n",
       "      <th>data22</th>\n",
       "      <th>data23</th>\n",
       "      <th>data24</th>\n",
       "      <th>data25</th>\n",
       "      <th>data26</th>\n",
       "      <th>data27</th>\n",
       "      <th>data28</th>\n",
       "      <th>data29</th>\n",
       "      <th>data30</th>\n",
       "      <th>Control</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   data1  data2  data3  data4  data5  data6  data7  data8     data9    data10  \\\n",
       "0    0.0    0.0    0.0    0.1   -0.1    0.0    0.0    0.4  0.100000  0.500000   \n",
       "1    0.0    0.0    0.0    0.0    0.0    1.0   -0.4   -0.6  0.000000  0.000000   \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.000000  0.000000   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.000000  0.000000   \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  0.954545  0.045455   \n",
       "\n",
       "   ...  data22  data23  data24  data25  data26  data27  data28  data29  \\\n",
       "0  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   data30  Control  \n",
       "0     0.0        1  \n",
       "1     0.0        0  \n",
       "2     0.0        0  \n",
       "3     0.0        0  \n",
       "4     0.0        0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset into a dataframe\n",
    "path = 'Preprocessed_AFData.xlsx' #Path\n",
    "df = pd.read_excel(path) #Turning into a dataframe\n",
    "print('shape of the dataframe:', np.shape(df))\n",
    "df.head() #Printing an example of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005062d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the train dataframe: (90000, 30)\n",
      "shape of the train label dataframe: (90000,)\n",
      "shape of the test dataframe: (30000, 30)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data set into training and test set\n",
    "train_set, rest_set = train_test_split(df, test_size=0.4, random_state=100) #Splitting into training and test part\n",
    "test_set, val_set = train_test_split(rest_set, test_size=0.5, random_state=42)\n",
    "\n",
    "#Separating signal from label\n",
    "X_train = train_set.iloc[:,:30].to_numpy() \n",
    "y_train = train_set.iloc[:,30].to_numpy()\n",
    "X_test = test_set.iloc[:,:30].to_numpy()\n",
    "y_test = test_set.iloc[:,30].to_numpy()\n",
    "X_val = val_set.iloc[:,:30].to_numpy()\n",
    "y_val = val_set.iloc[:,30].to_numpy()\n",
    "\n",
    "#Normalize the training data\n",
    "# scaler = StandardScaler() \n",
    "# print(X_train[1,:])\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# print(X_train[1,:])\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "print('shape of the train dataframe:', np.shape(X_train))\n",
    "print('shape of the train label dataframe:', np.shape(y_train))\n",
    "print('shape of the test dataframe:', np.shape(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29443108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bas_w\\Documents\\PYTHON\\Anaconda\\envs\\deeplearn_course\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n",
      "C:\\Users\\bas_w\\Documents\\PYTHON\\Anaconda\\envs\\deeplearn_course\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Creating DataLoader instances\n",
    "batch_size = 512\n",
    "\n",
    "#Creating a Dataset instance for the dataloader to call\n",
    "class dataset(Dataset):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = torch.tensor(x,dtype=torch.float32)\n",
    "        self.y = torch.tensor(y,dtype=torch.float32)\n",
    "        self.length = self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "y_val = torch.FloatTensor(y_val)\n",
    "\n",
    "train_set = dataset(X_train, y_train) #Training data set\n",
    "test_set = dataset(X_test, y_test) #Test data set\n",
    "val_set = dataset(X_val, y_val) #Validation data set\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                         batch_size=1,\n",
    "                                         shuffle=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                        batch_size=1,\n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4828361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network\n",
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64 , 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):  \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02602ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting some parameters\n",
    "lr = 0.001\n",
    "epochs = 200\n",
    "model = NeuralNet(in_channels = X_train.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "# criterion = nn.BCEWithLogitsLoss() # No weights\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.FloatTensor ([20 / 1])) # With weights\n",
    "save_path = 'Checkpoints/Saved_model_weights_with_class_weights20'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54a51778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy statistic\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40c795c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7388\n",
      "Epoch 001: | Loss: 2.25152 | Acc: 24.080\n",
      "Epoch 001: | Test Loss: 2.2104756892184416 | Test Acc: 24.627\n",
      "New model saved!!\n",
      "7389\n",
      "Epoch 002: | Loss: 2.17245 | Acc: 24.068\n",
      "Epoch 002: | Test Loss: 2.1966433256347973 | Test Acc: 24.630\n",
      "New model saved!!\n",
      "7390\n",
      "Epoch 003: | Loss: 2.15833 | Acc: 24.097\n",
      "Epoch 003: | Test Loss: 2.1870778719703354 | Test Acc: 24.633\n",
      "New model saved!!\n",
      "7393\n",
      "Epoch 004: | Loss: 2.15406 | Acc: 24.074\n",
      "Epoch 004: | Test Loss: 2.185498139665524 | Test Acc: 24.643\n",
      "New model saved!!\n",
      "7398\n",
      "Epoch 005: | Loss: 2.15094 | Acc: 24.114\n",
      "Epoch 005: | Test Loss: 2.1832016595502695 | Test Acc: 24.660\n",
      "New model saved!!\n",
      "7416\n",
      "Epoch 006: | Loss: 2.14902 | Acc: 24.159\n",
      "Epoch 006: | Test Loss: 2.180920368085305 | Test Acc: 24.720\n",
      "New model saved!!\n",
      "7420\n",
      "Epoch 007: | Loss: 2.14788 | Acc: 24.199\n",
      "Epoch 007: | Test Loss: 2.180075200297435 | Test Acc: 24.733\n",
      "New model saved!!\n",
      "7475\n",
      "Epoch 008: | Loss: 2.14728 | Acc: 24.284\n",
      "Epoch 008: | Test Loss: 2.1811959581454596 | Test Acc: 24.917\n",
      "7497\n",
      "Epoch 009: | Loss: 2.14598 | Acc: 24.420\n",
      "Epoch 009: | Test Loss: 2.1790970604916415 | Test Acc: 24.990\n",
      "New model saved!!\n",
      "7503\n",
      "Epoch 010: | Loss: 2.14506 | Acc: 24.489\n",
      "Epoch 010: | Test Loss: 2.1799454441527524 | Test Acc: 25.010\n",
      "7580\n",
      "Epoch 011: | Loss: 2.14567 | Acc: 24.653\n",
      "Epoch 011: | Test Loss: 2.1798325326561927 | Test Acc: 25.267\n",
      "7670\n",
      "Epoch 012: | Loss: 2.14550 | Acc: 24.943\n",
      "Epoch 012: | Test Loss: 2.180786019007365 | Test Acc: 25.567\n",
      "7803\n",
      "Epoch 013: | Loss: 2.14473 | Acc: 25.398\n",
      "Epoch 013: | Test Loss: 2.179625665394465 | Test Acc: 26.010\n",
      "7854\n",
      "Epoch 014: | Loss: 2.14405 | Acc: 25.761\n",
      "Epoch 014: | Test Loss: 2.179730099910498 | Test Acc: 26.180\n",
      "8007\n",
      "Epoch 015: | Loss: 2.14415 | Acc: 26.062\n",
      "Epoch 015: | Test Loss: 2.1796224546571574 | Test Acc: 26.690\n",
      "8146\n",
      "Epoch 016: | Loss: 2.14394 | Acc: 26.301\n",
      "Epoch 016: | Test Loss: 2.180384037933747 | Test Acc: 27.153\n",
      "8374\n",
      "Epoch 017: | Loss: 2.14437 | Acc: 26.778\n",
      "Epoch 017: | Test Loss: 2.181411341013511 | Test Acc: 27.913\n",
      "8717\n",
      "Epoch 018: | Loss: 2.14369 | Acc: 27.807\n",
      "Epoch 018: | Test Loss: 2.177533332337936 | Test Acc: 29.057\n",
      "New model saved!!\n",
      "8831\n",
      "Epoch 019: | Loss: 2.14301 | Acc: 28.693\n",
      "Epoch 019: | Test Loss: 2.180063415602843 | Test Acc: 29.437\n",
      "8374\n",
      "Epoch 020: | Loss: 2.14227 | Acc: 28.602\n",
      "Epoch 020: | Test Loss: 2.1757132426420847 | Test Acc: 27.913\n",
      "New model saved!!\n",
      "9094\n",
      "Epoch 021: | Loss: 2.14271 | Acc: 29.369\n",
      "Epoch 021: | Test Loss: 2.1776913453718025 | Test Acc: 30.313\n",
      "8478\n",
      "Epoch 022: | Loss: 2.14254 | Acc: 29.528\n",
      "Epoch 022: | Test Loss: 2.178828859615326 | Test Acc: 28.260\n",
      "8939\n",
      "Epoch 023: | Loss: 2.14252 | Acc: 30.034\n",
      "Epoch 023: | Test Loss: 2.178932101366917 | Test Acc: 29.797\n",
      "9661\n",
      "Epoch 024: | Loss: 2.14194 | Acc: 31.097\n",
      "Epoch 024: | Test Loss: 2.1756648930728435 | Test Acc: 32.203\n",
      "New model saved!!\n",
      "9241\n",
      "Epoch 025: | Loss: 2.14276 | Acc: 31.500\n",
      "Epoch 025: | Test Loss: 2.177448890690009 | Test Acc: 30.803\n",
      "10252\n",
      "Epoch 026: | Loss: 2.14237 | Acc: 32.858\n",
      "Epoch 026: | Test Loss: 2.180412299710512 | Test Acc: 34.173\n",
      "9816\n",
      "Epoch 027: | Loss: 2.14260 | Acc: 32.688\n",
      "Epoch 027: | Test Loss: 2.1799482707103093 | Test Acc: 32.720\n",
      "9575\n",
      "Epoch 028: | Loss: 2.14289 | Acc: 32.045\n",
      "Epoch 028: | Test Loss: 2.1791175642728806 | Test Acc: 31.917\n",
      "10412\n",
      "Epoch 029: | Loss: 2.14246 | Acc: 33.494\n",
      "Epoch 029: | Test Loss: 2.178429582373301 | Test Acc: 34.707\n",
      "10313\n",
      "Epoch 030: | Loss: 2.14158 | Acc: 34.920\n",
      "Epoch 030: | Test Loss: 2.177258696011702 | Test Acc: 34.377\n",
      "11819\n",
      "Epoch 031: | Loss: 2.14154 | Acc: 36.443\n",
      "Epoch 031: | Test Loss: 2.1783521252612275 | Test Acc: 39.397\n",
      "9675\n",
      "Epoch 032: | Loss: 2.14122 | Acc: 38.330\n",
      "Epoch 032: | Test Loss: 2.178494512599707 | Test Acc: 32.250\n",
      "11461\n",
      "Epoch 033: | Loss: 2.14175 | Acc: 37.585\n",
      "Epoch 033: | Test Loss: 2.1790089841246605 | Test Acc: 38.203\n",
      "11831\n",
      "Epoch 034: | Loss: 2.14189 | Acc: 39.398\n",
      "Epoch 034: | Test Loss: 2.176267211373647 | Test Acc: 39.437\n",
      "12120\n",
      "Epoch 035: | Loss: 2.14093 | Acc: 39.511\n",
      "Epoch 035: | Test Loss: 2.1772405849556127 | Test Acc: 40.400\n",
      "10551\n",
      "Epoch 036: | Loss: 2.14119 | Acc: 38.477\n",
      "Epoch 036: | Test Loss: 2.1792670303563275 | Test Acc: 35.170\n",
      "11617\n",
      "Epoch 037: | Loss: 2.14092 | Acc: 39.347\n",
      "Epoch 037: | Test Loss: 2.1777933921655017 | Test Acc: 38.723\n",
      "11357\n",
      "Epoch 038: | Loss: 2.14117 | Acc: 40.864\n",
      "Epoch 038: | Test Loss: 2.1765446515182654 | Test Acc: 37.857\n",
      "12145\n",
      "Epoch 039: | Loss: 2.14087 | Acc: 39.619\n",
      "Epoch 039: | Test Loss: 2.17960471214056 | Test Acc: 40.483\n",
      "11561\n",
      "Epoch 040: | Loss: 2.14106 | Acc: 41.943\n",
      "Epoch 040: | Test Loss: 2.1803166594664254 | Test Acc: 38.537\n",
      "10063\n",
      "Epoch 041: | Loss: 2.14096 | Acc: 41.943\n",
      "Epoch 041: | Test Loss: 2.1757329566001893 | Test Acc: 33.543\n",
      "13939\n",
      "Epoch 042: | Loss: 2.14114 | Acc: 43.182\n",
      "Epoch 042: | Test Loss: 2.1772616262078284 | Test Acc: 46.463\n",
      "15576\n",
      "Epoch 043: | Loss: 2.14112 | Acc: 45.364\n",
      "Epoch 043: | Test Loss: 2.1769534737030667 | Test Acc: 51.920\n",
      "15703\n",
      "Epoch 044: | Loss: 2.13956 | Acc: 45.585\n",
      "Epoch 044: | Test Loss: 2.178583986244599 | Test Acc: 52.343\n",
      "10964\n",
      "Epoch 045: | Loss: 2.13995 | Acc: 46.449\n",
      "Epoch 045: | Test Loss: 2.1767220953722797 | Test Acc: 36.547\n",
      "12161\n",
      "Epoch 046: | Loss: 2.13986 | Acc: 44.398\n",
      "Epoch 046: | Test Loss: 2.1808092675228914 | Test Acc: 40.537\n",
      "14304\n",
      "Epoch 047: | Loss: 2.14100 | Acc: 44.591\n",
      "Epoch 047: | Test Loss: 2.176342624604702 | Test Acc: 47.680\n",
      "15953\n",
      "Epoch 048: | Loss: 2.14038 | Acc: 48.807\n",
      "Epoch 048: | Test Loss: 2.1782202488859497 | Test Acc: 53.177\n",
      "14236\n",
      "Epoch 049: | Loss: 2.14001 | Acc: 49.301\n",
      "Epoch 049: | Test Loss: 2.1758944705148537 | Test Acc: 47.453\n",
      "15110\n",
      "Epoch 050: | Loss: 2.14006 | Acc: 47.193\n",
      "Epoch 050: | Test Loss: 2.1766813626845676 | Test Acc: 50.367\n",
      "14361\n",
      "Epoch 051: | Loss: 2.13902 | Acc: 47.364\n",
      "Epoch 051: | Test Loss: 2.177396358895302 | Test Acc: 47.870\n",
      "13799\n",
      "Epoch 052: | Loss: 2.13963 | Acc: 45.267\n",
      "Epoch 052: | Test Loss: 2.1780290205180646 | Test Acc: 45.997\n",
      "13309\n",
      "Epoch 053: | Loss: 2.14086 | Acc: 44.432\n",
      "Epoch 053: | Test Loss: 2.1770148472825688 | Test Acc: 44.363\n",
      "13017\n",
      "Epoch 054: | Loss: 2.14007 | Acc: 45.955\n",
      "Epoch 054: | Test Loss: 2.1785751687725385 | Test Acc: 43.390\n",
      "12702\n",
      "Epoch 055: | Loss: 2.13871 | Acc: 46.682\n",
      "Epoch 055: | Test Loss: 2.1776519191384316 | Test Acc: 42.340\n",
      "15080\n",
      "Epoch 056: | Loss: 2.14034 | Acc: 47.159\n",
      "Epoch 056: | Test Loss: 2.180174735794465 | Test Acc: 50.267\n",
      "13777\n",
      "Epoch 057: | Loss: 2.14079 | Acc: 48.545\n",
      "Epoch 057: | Test Loss: 2.1777314882914225 | Test Acc: 45.923\n",
      "14273\n",
      "Epoch 058: | Loss: 2.13917 | Acc: 47.915\n",
      "Epoch 058: | Test Loss: 2.176149362806479 | Test Acc: 47.577\n",
      "13889\n",
      "Epoch 059: | Loss: 2.13994 | Acc: 47.517\n",
      "Epoch 059: | Test Loss: 2.177259829248985 | Test Acc: 46.297\n",
      "14482\n",
      "Epoch 060: | Loss: 2.13971 | Acc: 48.085\n",
      "Epoch 060: | Test Loss: 2.177290380680561 | Test Acc: 48.273\n",
      "13526\n",
      "Epoch 061: | Loss: 2.14029 | Acc: 49.926\n",
      "Epoch 061: | Test Loss: 2.177291414187352 | Test Acc: 45.087\n",
      "16292\n",
      "Epoch 062: | Loss: 2.13994 | Acc: 51.409\n",
      "Epoch 062: | Test Loss: 2.1735571855644387 | Test Acc: 54.307\n",
      "New model saved!!\n",
      "15887\n",
      "Epoch 063: | Loss: 2.13932 | Acc: 49.659\n",
      "Epoch 063: | Test Loss: 2.1774192506432533 | Test Acc: 52.957\n",
      "14509\n",
      "Epoch 064: | Loss: 2.13982 | Acc: 50.312\n",
      "Epoch 064: | Test Loss: 2.1772625130911667 | Test Acc: 48.363\n",
      "16364\n",
      "Epoch 065: | Loss: 2.14022 | Acc: 49.602\n",
      "Epoch 065: | Test Loss: 2.2033617207050322 | Test Acc: 54.547\n",
      "16281\n",
      "Epoch 066: | Loss: 2.14009 | Acc: 51.574\n",
      "Epoch 066: | Test Loss: 2.1781883080899713 | Test Acc: 54.270\n",
      "17062\n",
      "Epoch 067: | Loss: 2.14025 | Acc: 52.483\n",
      "Epoch 067: | Test Loss: 2.1781445761442186 | Test Acc: 56.873\n",
      "16600\n",
      "Epoch 068: | Loss: 2.14007 | Acc: 54.602\n",
      "Epoch 068: | Test Loss: 2.1764003239492578 | Test Acc: 55.333\n",
      "16332\n",
      "Epoch 069: | Loss: 2.13959 | Acc: 55.915\n",
      "Epoch 069: | Test Loss: 2.1778692400753497 | Test Acc: 54.440\n",
      "18112\n",
      "Epoch 070: | Loss: 2.13965 | Acc: 56.784\n",
      "Epoch 070: | Test Loss: 2.1785338365912437 | Test Acc: 60.373\n",
      "16879\n",
      "Epoch 071: | Loss: 2.13933 | Acc: 56.324\n",
      "Epoch 071: | Test Loss: 2.17779121389389 | Test Acc: 56.263\n",
      "16619\n",
      "Epoch 072: | Loss: 2.13924 | Acc: 57.989\n",
      "Epoch 072: | Test Loss: 2.178321417639653 | Test Acc: 55.397\n",
      "16692\n",
      "Epoch 073: | Loss: 2.13948 | Acc: 57.744\n",
      "Epoch 073: | Test Loss: 2.179065366603931 | Test Acc: 55.640\n",
      "18456\n",
      "Epoch 074: | Loss: 2.13951 | Acc: 57.381\n",
      "Epoch 074: | Test Loss: 2.178788012923797 | Test Acc: 61.520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16546\n",
      "Epoch 075: | Loss: 2.13914 | Acc: 56.682\n",
      "Epoch 075: | Test Loss: 2.178442023080587 | Test Acc: 55.153\n",
      "19034\n",
      "Epoch 076: | Loss: 2.13899 | Acc: 58.455\n",
      "Epoch 076: | Test Loss: 2.176587982648611 | Test Acc: 63.447\n",
      "19141\n",
      "Epoch 077: | Loss: 2.13830 | Acc: 57.330\n",
      "Epoch 077: | Test Loss: 2.179162794651588 | Test Acc: 63.803\n",
      "20123\n",
      "Epoch 078: | Loss: 2.13924 | Acc: 57.903\n",
      "Epoch 078: | Test Loss: 2.1757369317869344 | Test Acc: 67.077\n",
      "18862\n",
      "Epoch 079: | Loss: 2.13913 | Acc: 58.824\n",
      "Epoch 079: | Test Loss: 2.1760750735998156 | Test Acc: 62.873\n",
      "15021\n",
      "Epoch 080: | Loss: 2.14032 | Acc: 53.716\n",
      "Epoch 080: | Test Loss: 2.1779377793391546 | Test Acc: 50.070\n",
      "16054\n",
      "Epoch 081: | Loss: 2.13902 | Acc: 52.597\n",
      "Epoch 081: | Test Loss: 2.180220209413767 | Test Acc: 53.513\n",
      "18927\n",
      "Epoch 082: | Loss: 2.13886 | Acc: 56.523\n",
      "Epoch 082: | Test Loss: 2.175904414665699 | Test Acc: 63.090\n",
      "19745\n",
      "Epoch 083: | Loss: 2.13796 | Acc: 58.562\n",
      "Epoch 083: | Test Loss: 2.1799532684286436 | Test Acc: 65.817\n",
      "17079\n",
      "Epoch 084: | Loss: 2.13957 | Acc: 58.284\n",
      "Epoch 084: | Test Loss: 2.179061645781994 | Test Acc: 56.930\n",
      "17853\n",
      "Epoch 085: | Loss: 2.13845 | Acc: 58.227\n",
      "Epoch 085: | Test Loss: 2.179113499669234 | Test Acc: 59.510\n",
      "18009\n",
      "Epoch 086: | Loss: 2.13881 | Acc: 58.949\n",
      "Epoch 086: | Test Loss: 2.177764859523376 | Test Acc: 60.030\n",
      "17485\n",
      "Epoch 087: | Loss: 2.13745 | Acc: 59.125\n",
      "Epoch 087: | Test Loss: 2.178178226876259 | Test Acc: 58.283\n",
      "18785\n",
      "Epoch 088: | Loss: 2.13878 | Acc: 60.676\n",
      "Epoch 088: | Test Loss: 2.1783859811743103 | Test Acc: 62.617\n",
      "17815\n",
      "Epoch 089: | Loss: 2.13831 | Acc: 60.830\n",
      "Epoch 089: | Test Loss: 2.176139972990751 | Test Acc: 59.383\n",
      "16949\n",
      "Epoch 090: | Loss: 2.13849 | Acc: 59.460\n",
      "Epoch 090: | Test Loss: 2.1777972164491812 | Test Acc: 56.497\n",
      "17681\n",
      "Epoch 091: | Loss: 2.13835 | Acc: 60.426\n",
      "Epoch 091: | Test Loss: 2.1788410460611183 | Test Acc: 58.937\n",
      "18227\n",
      "Epoch 092: | Loss: 2.13799 | Acc: 60.091\n",
      "Epoch 092: | Test Loss: 2.179755355334282 | Test Acc: 60.757\n",
      "18532\n",
      "Epoch 093: | Loss: 2.13855 | Acc: 59.091\n",
      "Epoch 093: | Test Loss: 2.1780130389809607 | Test Acc: 61.773\n",
      "18094\n",
      "Epoch 094: | Loss: 2.13852 | Acc: 59.091\n",
      "Epoch 094: | Test Loss: 2.1801164373318356 | Test Acc: 60.313\n",
      "18270\n",
      "Epoch 095: | Loss: 2.13826 | Acc: 59.989\n",
      "Epoch 095: | Test Loss: 2.177620448778073 | Test Acc: 60.900\n",
      "17594\n",
      "Epoch 096: | Loss: 2.13861 | Acc: 58.665\n",
      "Epoch 096: | Test Loss: 2.1779054657737413 | Test Acc: 58.647\n",
      "19268\n",
      "Epoch 097: | Loss: 2.13780 | Acc: 60.330\n",
      "Epoch 097: | Test Loss: 2.1749477132181325 | Test Acc: 64.227\n",
      "19794\n",
      "Epoch 098: | Loss: 2.13818 | Acc: 61.500\n",
      "Epoch 098: | Test Loss: 2.1770792179365954 | Test Acc: 65.980\n",
      "19426\n",
      "Epoch 099: | Loss: 2.13890 | Acc: 60.216\n",
      "Epoch 099: | Test Loss: 2.1757462172011532 | Test Acc: 64.753\n",
      "19117\n",
      "Epoch 100: | Loss: 2.13785 | Acc: 62.080\n",
      "Epoch 100: | Test Loss: 2.173558999749025 | Test Acc: 63.723\n",
      "17451\n",
      "Epoch 101: | Loss: 2.13725 | Acc: 60.472\n",
      "Epoch 101: | Test Loss: 2.175320388029019 | Test Acc: 58.170\n",
      "18847\n",
      "Epoch 102: | Loss: 2.13830 | Acc: 60.386\n",
      "Epoch 102: | Test Loss: 2.176679883245627 | Test Acc: 62.823\n",
      "19733\n",
      "Epoch 103: | Loss: 2.13855 | Acc: 61.847\n",
      "Epoch 103: | Test Loss: 2.1798274786671 | Test Acc: 65.777\n",
      "17259\n",
      "Epoch 104: | Loss: 2.13757 | Acc: 62.125\n",
      "Epoch 104: | Test Loss: 2.1808135130822657 | Test Acc: 57.530\n",
      "18207\n",
      "Epoch 105: | Loss: 2.13841 | Acc: 59.830\n",
      "Epoch 105: | Test Loss: 2.177772537893057 | Test Acc: 60.690\n",
      "18242\n",
      "Epoch 106: | Loss: 2.13827 | Acc: 60.091\n",
      "Epoch 106: | Test Loss: 2.1785363778034847 | Test Acc: 60.807\n",
      "17534\n",
      "Epoch 107: | Loss: 2.13843 | Acc: 59.898\n",
      "Epoch 107: | Test Loss: 2.179798260863622 | Test Acc: 58.447\n",
      "18245\n",
      "Epoch 108: | Loss: 2.13802 | Acc: 60.466\n",
      "Epoch 108: | Test Loss: 2.1737070750633873 | Test Acc: 60.817\n",
      "19620\n",
      "Epoch 109: | Loss: 2.13725 | Acc: 61.892\n",
      "Epoch 109: | Test Loss: 2.1788091310540834 | Test Acc: 65.400\n",
      "18974\n",
      "Epoch 110: | Loss: 2.13776 | Acc: 63.676\n",
      "Epoch 110: | Test Loss: 2.1768934581001598 | Test Acc: 63.247\n",
      "18682\n",
      "Epoch 111: | Loss: 2.13668 | Acc: 62.773\n",
      "Epoch 111: | Test Loss: 2.1757983386933804 | Test Acc: 62.273\n",
      "19888\n",
      "Epoch 112: | Loss: 2.13793 | Acc: 63.483\n",
      "Epoch 112: | Test Loss: 2.1745816814641157 | Test Acc: 66.293\n",
      "19053\n",
      "Epoch 113: | Loss: 2.13724 | Acc: 63.557\n",
      "Epoch 113: | Test Loss: 2.1795904228687286 | Test Acc: 63.510\n",
      "17911\n",
      "Epoch 114: | Loss: 2.13774 | Acc: 62.642\n",
      "Epoch 114: | Test Loss: 2.1767338504036267 | Test Acc: 59.703\n",
      "20087\n",
      "Epoch 115: | Loss: 2.13750 | Acc: 62.278\n",
      "Epoch 115: | Test Loss: 2.176907285308838 | Test Acc: 66.957\n",
      "18909\n",
      "Epoch 116: | Loss: 2.13733 | Acc: 60.614\n",
      "Epoch 116: | Test Loss: 2.1756663947681587 | Test Acc: 63.030\n",
      "17471\n",
      "Epoch 117: | Loss: 2.13715 | Acc: 59.182\n",
      "Epoch 117: | Test Loss: 2.1813292387584844 | Test Acc: 58.237\n",
      "18480\n",
      "Epoch 118: | Loss: 2.13821 | Acc: 62.932\n",
      "Epoch 118: | Test Loss: 2.1807157529215018 | Test Acc: 61.600\n",
      "17842\n",
      "Epoch 119: | Loss: 2.13788 | Acc: 60.568\n",
      "Epoch 119: | Test Loss: 2.1809454162478445 | Test Acc: 59.473\n",
      "18705\n",
      "Epoch 120: | Loss: 2.13704 | Acc: 60.500\n",
      "Epoch 120: | Test Loss: 2.178964845108986 | Test Acc: 62.350\n",
      "18022\n",
      "Epoch 121: | Loss: 2.13786 | Acc: 60.932\n",
      "Epoch 121: | Test Loss: 2.1779985944608846 | Test Acc: 60.073\n",
      "18529\n",
      "Epoch 122: | Loss: 2.13666 | Acc: 61.239\n",
      "Epoch 122: | Test Loss: 2.1749411660174527 | Test Acc: 61.763\n",
      "19641\n",
      "Epoch 123: | Loss: 2.13768 | Acc: 61.676\n",
      "Epoch 123: | Test Loss: 2.1777880958398184 | Test Acc: 65.470\n",
      "18197\n",
      "Epoch 124: | Loss: 2.13793 | Acc: 61.472\n",
      "Epoch 124: | Test Loss: 2.175794212861856 | Test Acc: 60.657\n",
      "19158\n",
      "Epoch 125: | Loss: 2.13795 | Acc: 59.017\n",
      "Epoch 125: | Test Loss: 2.1783492237428823 | Test Acc: 63.860\n",
      "18801\n",
      "Epoch 126: | Loss: 2.13699 | Acc: 61.000\n",
      "Epoch 126: | Test Loss: 2.174258483703931 | Test Acc: 62.670\n",
      "19288\n",
      "Epoch 127: | Loss: 2.13860 | Acc: 60.818\n",
      "Epoch 127: | Test Loss: 2.1787670192817847 | Test Acc: 64.293\n",
      "19153\n",
      "Epoch 128: | Loss: 2.13835 | Acc: 61.051\n",
      "Epoch 128: | Test Loss: 2.179554930227995 | Test Acc: 63.843\n",
      "17976\n",
      "Epoch 129: | Loss: 2.13851 | Acc: 61.472\n",
      "Epoch 129: | Test Loss: 2.179326487763723 | Test Acc: 59.920\n",
      "19096\n",
      "Epoch 130: | Loss: 2.13883 | Acc: 60.989\n",
      "Epoch 130: | Test Loss: 2.177531250735124 | Test Acc: 63.653\n",
      "19522\n",
      "Epoch 131: | Loss: 2.13807 | Acc: 60.398\n",
      "Epoch 131: | Test Loss: 2.1787106427987415 | Test Acc: 65.073\n",
      "18644\n",
      "Epoch 132: | Loss: 2.13698 | Acc: 62.153\n",
      "Epoch 132: | Test Loss: 2.1754241448144116 | Test Acc: 62.147\n",
      "19288\n",
      "Epoch 133: | Loss: 2.13752 | Acc: 61.693\n",
      "Epoch 133: | Test Loss: 2.1774768237968285 | Test Acc: 64.293\n",
      "18777\n",
      "Epoch 134: | Loss: 2.13743 | Acc: 61.920\n",
      "Epoch 134: | Test Loss: 2.180121198930343 | Test Acc: 62.590\n",
      "16409\n",
      "Epoch 135: | Loss: 2.13764 | Acc: 60.511\n",
      "Epoch 135: | Test Loss: 2.179636357567708 | Test Acc: 54.697\n",
      "18354\n",
      "Epoch 136: | Loss: 2.13815 | Acc: 61.443\n",
      "Epoch 136: | Test Loss: 2.1799256448805333 | Test Acc: 61.180\n",
      "16781\n",
      "Epoch 137: | Loss: 2.13829 | Acc: 62.938\n",
      "Epoch 137: | Test Loss: 2.1790837523718674 | Test Acc: 55.937\n",
      "19355\n",
      "Epoch 138: | Loss: 2.13836 | Acc: 60.080\n",
      "Epoch 138: | Test Loss: 2.1772237889409065 | Test Acc: 64.517\n",
      "19377\n",
      "Epoch 139: | Loss: 2.13780 | Acc: 62.051\n",
      "Epoch 139: | Test Loss: 2.1792733943005405 | Test Acc: 64.590\n",
      "19243\n",
      "Epoch 140: | Loss: 2.13800 | Acc: 61.926\n",
      "Epoch 140: | Test Loss: 2.181027052642902 | Test Acc: 64.143\n",
      "18343\n",
      "Epoch 141: | Loss: 2.13723 | Acc: 63.369\n",
      "Epoch 141: | Test Loss: 2.178749648509423 | Test Acc: 61.143\n",
      "19086\n",
      "Epoch 142: | Loss: 2.13620 | Acc: 61.511\n",
      "Epoch 142: | Test Loss: 2.1776624780337017 | Test Acc: 63.620\n",
      "17298\n",
      "Epoch 143: | Loss: 2.13766 | Acc: 59.239\n",
      "Epoch 143: | Test Loss: 2.17972198831439 | Test Acc: 57.660\n",
      "19447\n",
      "Epoch 144: | Loss: 2.13750 | Acc: 62.148\n",
      "Epoch 144: | Test Loss: 2.1753743200937907 | Test Acc: 64.823\n",
      "18963\n",
      "Epoch 145: | Loss: 2.13747 | Acc: 61.608\n",
      "Epoch 145: | Test Loss: 2.1804322417835396 | Test Acc: 63.210\n",
      "18513\n",
      "Epoch 146: | Loss: 2.13769 | Acc: 61.682\n",
      "Epoch 146: | Test Loss: 2.1763928363780183 | Test Acc: 61.710\n",
      "19188\n",
      "Epoch 147: | Loss: 2.13792 | Acc: 61.688\n",
      "Epoch 147: | Test Loss: 2.1799298138995966 | Test Acc: 63.960\n",
      "18691\n",
      "Epoch 148: | Loss: 2.13687 | Acc: 62.472\n",
      "Epoch 148: | Test Loss: 2.1789670412083466 | Test Acc: 62.303\n",
      "17358\n",
      "Epoch 149: | Loss: 2.13686 | Acc: 61.574\n",
      "Epoch 149: | Test Loss: 2.179212227469683 | Test Acc: 57.860\n",
      "19615\n",
      "Epoch 150: | Loss: 2.13688 | Acc: 59.659\n",
      "Epoch 150: | Test Loss: 2.1815719197074572 | Test Acc: 65.383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18251\n",
      "Epoch 151: | Loss: 2.13663 | Acc: 59.886\n",
      "Epoch 151: | Test Loss: 2.1807381723622483 | Test Acc: 60.837\n",
      "18067\n",
      "Epoch 152: | Loss: 2.13656 | Acc: 60.540\n",
      "Epoch 152: | Test Loss: 2.178376179675261 | Test Acc: 60.223\n",
      "19248\n",
      "Epoch 153: | Loss: 2.13789 | Acc: 59.830\n",
      "Epoch 153: | Test Loss: 2.1780355472822985 | Test Acc: 64.160\n",
      "19146\n",
      "Epoch 154: | Loss: 2.13705 | Acc: 59.307\n",
      "Epoch 154: | Test Loss: 2.1776465440134207 | Test Acc: 63.820\n",
      "17705\n",
      "Epoch 155: | Loss: 2.13802 | Acc: 58.398\n",
      "Epoch 155: | Test Loss: 2.1790512676695983 | Test Acc: 59.017\n",
      "18067\n",
      "Epoch 156: | Loss: 2.13786 | Acc: 59.591\n",
      "Epoch 156: | Test Loss: 2.176260196371873 | Test Acc: 60.223\n",
      "17648\n",
      "Epoch 157: | Loss: 2.13641 | Acc: 60.170\n",
      "Epoch 157: | Test Loss: 2.181990771694978 | Test Acc: 58.827\n",
      "18091\n",
      "Epoch 158: | Loss: 2.13750 | Acc: 58.699\n",
      "Epoch 158: | Test Loss: 2.181221914591392 | Test Acc: 60.303\n",
      "17953\n",
      "Epoch 159: | Loss: 2.13714 | Acc: 58.881\n",
      "Epoch 159: | Test Loss: 2.1789455868800482 | Test Acc: 59.843\n",
      "18614\n",
      "Epoch 160: | Loss: 2.13596 | Acc: 60.580\n",
      "Epoch 160: | Test Loss: 2.1783021181901296 | Test Acc: 62.047\n",
      "19354\n",
      "Epoch 161: | Loss: 2.13676 | Acc: 58.989\n",
      "Epoch 161: | Test Loss: 2.1804936799943446 | Test Acc: 64.513\n",
      "17604\n",
      "Epoch 162: | Loss: 2.13751 | Acc: 60.256\n",
      "Epoch 162: | Test Loss: 2.1817750976622103 | Test Acc: 58.680\n",
      "18320\n",
      "Epoch 163: | Loss: 2.13668 | Acc: 60.108\n",
      "Epoch 163: | Test Loss: 2.180045702826977 | Test Acc: 61.067\n",
      "17506\n",
      "Epoch 164: | Loss: 2.13646 | Acc: 58.835\n",
      "Epoch 164: | Test Loss: 2.181026092761755 | Test Acc: 58.353\n",
      "17275\n",
      "Epoch 165: | Loss: 2.13741 | Acc: 58.534\n",
      "Epoch 165: | Test Loss: 2.1804554498334725 | Test Acc: 57.583\n",
      "18598\n",
      "Epoch 166: | Loss: 2.13711 | Acc: 59.790\n",
      "Epoch 166: | Test Loss: 2.1780379740794498 | Test Acc: 61.993\n",
      "18501\n",
      "Epoch 167: | Loss: 2.13747 | Acc: 60.636\n",
      "Epoch 167: | Test Loss: 2.1800593239049117 | Test Acc: 61.670\n",
      "18480\n",
      "Epoch 168: | Loss: 2.13730 | Acc: 62.324\n",
      "Epoch 168: | Test Loss: 2.1763300965368746 | Test Acc: 61.600\n",
      "17422\n",
      "Epoch 169: | Loss: 2.13629 | Acc: 61.602\n",
      "Epoch 169: | Test Loss: 2.181819383394718 | Test Acc: 58.073\n",
      "19615\n",
      "Epoch 170: | Loss: 2.13622 | Acc: 63.034\n",
      "Epoch 170: | Test Loss: 2.181600334598621 | Test Acc: 65.383\n",
      "19421\n",
      "Epoch 171: | Loss: 2.13644 | Acc: 63.739\n",
      "Epoch 171: | Test Loss: 2.179779284040133 | Test Acc: 64.737\n",
      "19491\n",
      "Epoch 172: | Loss: 2.13620 | Acc: 60.841\n",
      "Epoch 172: | Test Loss: 2.1807428269306817 | Test Acc: 64.970\n",
      "16936\n",
      "Epoch 173: | Loss: 2.13637 | Acc: 63.653\n",
      "Epoch 173: | Test Loss: 2.1798367095073066 | Test Acc: 56.453\n",
      "17479\n",
      "Epoch 174: | Loss: 2.13737 | Acc: 60.136\n",
      "Epoch 174: | Test Loss: 2.1820600735783575 | Test Acc: 58.263\n",
      "18142\n",
      "Epoch 175: | Loss: 2.13738 | Acc: 60.608\n",
      "Epoch 175: | Test Loss: 2.175599816095829 | Test Acc: 60.473\n",
      "17402\n",
      "Epoch 176: | Loss: 2.13627 | Acc: 61.080\n",
      "Epoch 176: | Test Loss: 2.1799353071113425 | Test Acc: 58.007\n",
      "19069\n",
      "Epoch 177: | Loss: 2.13788 | Acc: 61.392\n",
      "Epoch 177: | Test Loss: 2.177241415921847 | Test Acc: 63.563\n",
      "20541\n",
      "Epoch 178: | Loss: 2.13639 | Acc: 63.818\n",
      "Epoch 178: | Test Loss: 2.177439797616005 | Test Acc: 68.470\n",
      "19768\n",
      "Epoch 179: | Loss: 2.13633 | Acc: 64.318\n",
      "Epoch 179: | Test Loss: 2.175116312813759 | Test Acc: 65.893\n",
      "19149\n",
      "Epoch 180: | Loss: 2.13569 | Acc: 63.489\n",
      "Epoch 180: | Test Loss: 2.1779219209512073 | Test Acc: 63.830\n",
      "18904\n",
      "Epoch 181: | Loss: 2.13598 | Acc: 61.830\n",
      "Epoch 181: | Test Loss: 2.1762588241418204 | Test Acc: 63.013\n",
      "17842\n",
      "Epoch 182: | Loss: 2.13609 | Acc: 61.625\n",
      "Epoch 182: | Test Loss: 2.1783896169920762 | Test Acc: 59.473\n",
      "19665\n",
      "Epoch 183: | Loss: 2.13649 | Acc: 61.540\n",
      "Epoch 183: | Test Loss: 2.181255063736439 | Test Acc: 65.550\n",
      "17733\n",
      "Epoch 184: | Loss: 2.13694 | Acc: 61.290\n",
      "Epoch 184: | Test Loss: 2.186079750827948 | Test Acc: 59.110\n",
      "19635\n",
      "Epoch 185: | Loss: 2.13643 | Acc: 60.290\n",
      "Epoch 185: | Test Loss: 2.1760109462877115 | Test Acc: 65.450\n",
      "19036\n",
      "Epoch 186: | Loss: 2.13711 | Acc: 59.938\n",
      "Epoch 186: | Test Loss: 2.1748561030765377 | Test Acc: 63.453\n",
      "18030\n",
      "Epoch 187: | Loss: 2.13737 | Acc: 60.392\n",
      "Epoch 187: | Test Loss: 2.1784697974006333 | Test Acc: 60.100\n",
      "17408\n",
      "Epoch 188: | Loss: 2.13728 | Acc: 59.915\n",
      "Epoch 188: | Test Loss: 2.178753296049436 | Test Acc: 58.027\n",
      "19418\n",
      "Epoch 189: | Loss: 2.13664 | Acc: 58.403\n",
      "Epoch 189: | Test Loss: 2.1791085975944995 | Test Acc: 64.727\n",
      "18016\n",
      "Epoch 190: | Loss: 2.13612 | Acc: 58.199\n",
      "Epoch 190: | Test Loss: 2.1763194833636286 | Test Acc: 60.053\n",
      "16932\n",
      "Epoch 191: | Loss: 2.13540 | Acc: 57.761\n",
      "Epoch 191: | Test Loss: 2.174669638725122 | Test Acc: 56.440\n",
      "17024\n",
      "Epoch 192: | Loss: 2.13593 | Acc: 57.500\n",
      "Epoch 192: | Test Loss: 2.1767503435949482 | Test Acc: 56.747\n",
      "18952\n",
      "Epoch 193: | Loss: 2.13729 | Acc: 57.608\n",
      "Epoch 193: | Test Loss: 2.178076047760248 | Test Acc: 63.173\n",
      "17841\n",
      "Epoch 194: | Loss: 2.13664 | Acc: 57.057\n",
      "Epoch 194: | Test Loss: 2.175423799119393 | Test Acc: 59.470\n",
      "17069\n",
      "Epoch 195: | Loss: 2.13528 | Acc: 55.523\n",
      "Epoch 195: | Test Loss: 2.1802869667768476 | Test Acc: 56.897\n",
      "16483\n",
      "Epoch 196: | Loss: 2.13714 | Acc: 56.807\n",
      "Epoch 196: | Test Loss: 2.179463444519043 | Test Acc: 54.943\n",
      "17858\n",
      "Epoch 197: | Loss: 2.13588 | Acc: 56.011\n",
      "Epoch 197: | Test Loss: 2.1779903708159924 | Test Acc: 59.527\n",
      "16039\n",
      "Epoch 198: | Loss: 2.13592 | Acc: 55.494\n",
      "Epoch 198: | Test Loss: 2.1819221093297005 | Test Acc: 53.463\n",
      "15601\n",
      "Epoch 199: | Loss: 2.13675 | Acc: 55.631\n",
      "Epoch 199: | Test Loss: 2.18103995141983 | Test Acc: 52.003\n",
      "15793\n",
      "Epoch 200: | Loss: 2.13646 | Acc: 53.085\n",
      "Epoch 200: | Test Loss: 2.1827948469738168 | Test Acc: 52.643\n"
     ]
    }
   ],
   "source": [
    "# Training the network\n",
    "start = time.time()\n",
    "accuracy_best = -1\n",
    "best_loss = 99999\n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "for i in range(1, epochs+1):\n",
    "    #Training the network\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        \n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        acc = binary_acc(y_pred, y_batch.unsqueeze(1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    #Evaluating the network\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    test_epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_test_pred = model(X_batch)\n",
    "            test_loss = criterion(y_test_pred, y_batch.unsqueeze(1))\n",
    "            y_test_pred = torch.sigmoid(y_test_pred)\n",
    "            y_pred_tag = torch.round(y_test_pred).item()\n",
    "            test_epoch_loss += test_loss.item()\n",
    "            if y_pred_tag == y_batch.item():\n",
    "                correct += 1\n",
    "    \n",
    "    print(correct)       \n",
    "    accuracy = 100 * correct / len(test_set)\n",
    "    print(f'Epoch {i+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f}')\n",
    "    print(f'Epoch {i+0:03}: | Test Loss: {test_epoch_loss/len(test_loader)} | Test Acc: {accuracy:.3f}')\n",
    "    if test_epoch_loss < best_loss:\n",
    "        accuracy_best = accuracy\n",
    "        best_loss = test_epoch_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print('New model saved!!')\n",
    "        \n",
    "    train_loss.append(epoch_loss/len(train_loader))\n",
    "    train_acc.append(epoch_acc/len(train_loader))\n",
    "    test_acc.append(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f30830c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16155\n",
      "53.85\n"
     ]
    }
   ],
   "source": [
    "# Making predictions on validation set\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in val_loader:\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_test_pred = torch.sigmoid(y_test_pred)\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_true_list.append(y_batch.item())\n",
    "        if y_pred_tag == y_batch.item():\n",
    "            correct += 1\n",
    "            \n",
    "    accuracy = 100 * correct / len(test_set)\n",
    "    print(correct)\n",
    "    print(accuracy)\n",
    "\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "414c350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Label: 0       0.99      0.39      0.56     22545\n",
      "    Label: 1       0.35      0.98      0.51      7455\n",
      "\n",
      "    accuracy                           0.54     30000\n",
      "   macro avg       0.67      0.69      0.54     30000\n",
      "weighted avg       0.83      0.54      0.55     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_names = ['Label: 0','Label: 1']\n",
    "print(classification_report(y_true_list, y_pred_list,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc80958b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54df267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
